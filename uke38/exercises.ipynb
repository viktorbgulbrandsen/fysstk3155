{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccb7c43",
   "metadata": {},
   "source": [
    "## Use the books!\n",
    "\n",
    "This week deals with various mean values and variances in linear regression methods (here it may be useful to look up chapter 3, equation (3.8) of [Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer](https://www.springer.com/gp/book/9780387848570)).\n",
    "\n",
    "For more discussions on Ridge regression and calculation of expectation values, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "\n",
    "The exercises this week are also a part of project 1 and can be reused in the theory part of the project.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "We assume that there exists a continuous function $f(\\boldsymbol{x})$ and a normal distributed error $\\boldsymbol{\\varepsilon}\\sim N(0, \\sigma^2)$ which describes our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898dc17",
   "metadata": {},
   "source": [
    "$$\n",
    "A1: \\quad \\boldsymbol{y} = f(\\boldsymbol{x})+\\boldsymbol{\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa6ea1",
   "metadata": {},
   "source": [
    "We further assume that this continous function can be modeled with a linear model $\\mathbf{\\tilde{y}}$ of some features $\\mathbf{X}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76423d3",
   "metadata": {},
   "source": [
    "$$\n",
    "A2: \\quad \\boldsymbol{y} = \\boldsymbol{\\tilde{y}} + \\boldsymbol{\\varepsilon} = \\boldsymbol{X}\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a63676",
   "metadata": {},
   "source": [
    "We therefore get that our data $\\boldsymbol{y}$ has an expectation value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$, that is $\\boldsymbol{y}$ follows a normal distribution with mean value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fd365",
   "metadata": {},
   "source": [
    "## Exercise 1: Expectation values for ordinary least squares expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1756d78",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ show that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9f959",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\boldsymbol{\\beta}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f77883",
   "metadata": {},
   "source": [
    "We start with the definition of the OLS estimator\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{OLS} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "From the data generating process we have\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \n",
    "\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "Insert this expression for $\\boldsymbol{y}$ into the estimator (this step is using the assumption (A1) that the  ideal $\\beta$ is expressed from $X$ where only noise is stochastic)\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{OLS} = (X^\\top X)^{-1} X^\\top (X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}).\n",
    "$$\n",
    "\n",
    "Expand the product:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{OLS} \n",
    "= (X^\\top X)^{-1} X^\\top X \\boldsymbol{\\beta} \\;+\\; (X^\\top X)^{-1} X^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{OLS} \n",
    "= \\boldsymbol{\\beta} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "\n",
    "Now take the expectation, using linearity of expectation and the fact that $\\boldsymbol{\\beta}$ is non-stochastic while $\\mathbb{E}[\\boldsymbol{\\epsilon}] = 0$:\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{OLS}]\n",
    "= \\mathbb{E}[\\boldsymbol{\\beta} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\epsilon}].\n",
    "$$\n",
    "\n",
    "Since $\\boldsymbol{\\beta}$ is non-stochastic and $\\mathbb{E}[\\boldsymbol{\\epsilon}] = 0$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{OLS}] \n",
    "= \\boldsymbol{\\beta} + (X^\\top X)^{-1} X^\\top \\,\\mathbb{E}[\\boldsymbol{\\epsilon}],\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{OLS}] = \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "Hence, the OLS estimator is **unbiased**:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{OLS}] = \\boldsymbol{\\beta}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742508fb",
   "metadata": {},
   "source": [
    "**b)** Show that the variance of $\\boldsymbol{\\hat{\\beta}_{OLS}}$ is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b1722",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\sigma^2 \\, (\\mathbf{X}^{T} \\mathbf{X})^{-1}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a0312",
   "metadata": {},
   "source": [
    "We start from the expression for the OLS estimator obtained above:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{OLS} \n",
    "= \\boldsymbol{\\beta} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "\n",
    "The variance operator ignores constants, so only the second term contributes. Thus,\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{OLS}) \n",
    "= \\operatorname{Var}\\!\\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\epsilon} \\right).\n",
    "$$\n",
    "\n",
    "Factor out the non-random matrices:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{OLS}) \n",
    "= (X^\\top X)^{-1} X^\\top \\, \\operatorname{Var}(\\boldsymbol{\\epsilon}) \\, X (X^\\top X)^{-1}.\n",
    "$$\n",
    "\n",
    "From the data generating assumption we know\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 I.\n",
    "$$\n",
    "\n",
    "Substitute this:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{OLS}) \n",
    "= (X^\\top X)^{-1} X^\\top \\, (\\sigma^2 I) \\, X (X^\\top X)^{-1}.\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{OLS}) \n",
    "= \\sigma^2 (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1}.\n",
    "$$\n",
    "\n",
    "Since $X^\\top X$ is symmetric and invertible (under the Gauss-Markov assumptions, i.e no perfect multicollinearity / full column rank assumption, (Wikipedia contributors, 2025)).\n",
    "):\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{OLS}) \n",
    "= \\sigma^2 (X^\\top X)^{-1}.\n",
    "$$\n",
    "\n",
    "Hence, we have shown that:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{OLS}) = \\sigma^2 (X^\\top X)^{-1}.\n",
    "$$\n",
    "\n",
    "Reference:\n",
    "\n",
    "Wikipedia contributors. (2025, March 24). Gauss–Markov theorem. In Wikipedia. Retrieved September 10, 2025, from https://en.wikipedia.org/w/index.php?title=Gauss%E2%80%93Markov_theorem&oldid=1282157188"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3a74f",
   "metadata": {},
   "source": [
    "## Exercise 2: Expectation values for Ridge regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914f6e0",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{Ridge}}$ show that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874b80e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big]=(\\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I}_{pp})^{-1} (\\mathbf{X}^{\\top} \\mathbf{X})\\boldsymbol{\\beta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ce400",
   "metadata": {},
   "source": [
    "We start with the definition of the Ridge estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{Ridge} \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top \\boldsymbol{y},\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is the regularization parameter and $I_p$ is the $p \\times p$ identity matrix.\n",
    "\n",
    "From the data generating process we have\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \n",
    "\\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "Insert this into the estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{Ridge} \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top (X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}).\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{Ridge} \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top X \\boldsymbol{\\beta} \n",
    "\\;+\\; (X^\\top X + \\lambda I_p)^{-1} X^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "\n",
    "Now take the expectation, using linearity and $\\mathbb{E}[\\boldsymbol{\\epsilon}] = 0$ so that last term zeros:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{Ridge}]\n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top X \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "Thus we have shown:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{Ridge}] \n",
    "= (X^\\top X + \\lambda I_p)^{-1} (X^\\top X) \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "Note: \n",
    "\n",
    "Therefore by inspection of the expression we see\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{Ridge}] = \\mathbb{E}[\\hat{\\boldsymbol{\\beta}}_{OLS}] \\iff \\lambda = 0\n",
    "$$\n",
    "Hence for any $\\lambda > 0$ the Ridge estimator is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206dab3",
   "metadata": {},
   "source": [
    "**b)** Why do we say that Ridge regression gives a biased estimate? Is this a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a17682",
   "metadata": {},
   "source": [
    "According to the bias–variance tradeoff (Hastie, Tibshirani, and Friedman, 2009, p.223), introducing bias can be beneficial.  \n",
    "Ridge adds bias by shrinking the estimator coefficients, but in turn it reduces variance — notably when predictors are highly correlated and the feature matrix $X$ is close to singular.  \n",
    "\n",
    "Its strength lies under conditions of multicollinearity or high dimensionality, where it effectively trades some bias for lower variance.  \n",
    "In practical settings, this is valuable: measurement noise, limited sample sizes, or strongly correlated features can make the OLS estimator oversensitive and unstable. Ridge stabilizes the solution, lowering mean squared error even though it is biased.\n",
    "\n",
    "Reference:\n",
    "\n",
    "Trevor Hastie, Robert Tibshirani, and Jerome Friedman. \n",
    "The Elements of Statistical Learning: Data Mining, Inference, and Prediction}. \n",
    "Second Edition. Springer, 2009.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a1e15",
   "metadata": {},
   "source": [
    "**c)** Show that the variance is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20661707",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}]=\\sigma^2[  \\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}  \\mathbf{X}^{T}\\mathbf{X} \\{ [  \\mathbf{X}^{\\top} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}\\}^{T}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac8cbd",
   "metadata": {},
   "source": [
    "We see that if the parameter $\\lambda$ goes to infinity then the variance of the Ridge parameters $\\boldsymbol{\\beta}$ goes to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a3a66",
   "metadata": {},
   "source": [
    "The proof in structure is similar to the others. \n",
    "We start from the expression of the Ridge estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{Ridge} \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "From the data generating process we have\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \n",
    "\\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "Insert this into the estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{Ridge} \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top (X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}).\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{Ridge} \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top X \\boldsymbol{\\beta} \n",
    "\\;+\\; (X^\\top X + \\lambda I_p)^{-1} X^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "\n",
    "The first term is deterministic by assumption, hence only the second contributes to the variance:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{Ridge}) \n",
    "= \\operatorname{Var}\\!\\left( (X^\\top X + \\lambda I_p)^{-1} X^\\top \\boldsymbol{\\epsilon} \\right).\n",
    "$$\n",
    "\n",
    "Using the variance matrix rule for independent \"errors\" $\\operatorname{Var}(A\\boldsymbol{\\epsilon}) = A \\, \\operatorname{Var}(\\boldsymbol{\\epsilon}) \\, A^\\top$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{Ridge}) \n",
    "= (X^\\top X + \\lambda I_p)^{-1} X^\\top \\, \\operatorname{Var}(\\boldsymbol{\\epsilon}) \\, X \\big[(X^\\top X + \\lambda I_p)^{-1}\\big]^\\top.\n",
    "$$\n",
    "\n",
    "Since $\\operatorname{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 I$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{Ridge}) \n",
    "= \\sigma^2 (X^\\top X + \\lambda I_p)^{-1} X^\\top X \\big[(X^\\top X + \\lambda I_p)^{-1}\\big]^\\top.\n",
    "$$\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "\n",
    "We can now see that as $\\lambda \\to \\infty$, the shrinkage term $(X^\\top X + \\lambda I_p)^{-1}$ goes to zero, so\n",
    "\n",
    "$$\n",
    "\\lim_{\\lambda \\to \\infty} \\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}_{Ridge}) = 0.\n",
    "$$\n",
    "\n",
    "Such a dynamic clearly invokes the general bias-variance tradeoff, as $\\lambda$ increases so does bias, concurrently the estimator variance shrinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6b450",
   "metadata": {},
   "source": [
    "## Exercise 3: Deriving the expression for the Bias-Variance Trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd35dd6",
   "metadata": {},
   "source": [
    "The aim of this exercise is to derive the equations for the bias-variance tradeoff to be used in project 1.\n",
    "\n",
    "The parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ are found by optimizing the mean squared error via the so-called cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eea135",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta}) =\\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdfde4",
   "metadata": {},
   "source": [
    "**a)** Show that you can rewrite this into an expression which contains\n",
    "\n",
    "- the variance of the model (the variance term)\n",
    "- the expected deviation of the mean of the model from the true data (the bias term)\n",
    "- the variance of the noise\n",
    "\n",
    "In other words, show that:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ea52e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\\mathrm{Bias}[\\tilde{y}]+\\mathrm{var}[\\tilde{y}]+\\sigma^2,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845841b0",
   "metadata": {},
   "source": [
    "with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27cbcb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{Bias}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\boldsymbol{y}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right],\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ddb8e",
   "metadata": {},
   "source": [
    "and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc57630",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\tilde{\\boldsymbol{y}}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right]=\\frac{1}{n}\\sum_i(\\tilde{y}_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4fb77",
   "metadata": {},
   "source": [
    "---\n",
    "We want to decompose the mean squared error\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{y})^2].\n",
    "$$\n",
    "\n",
    "Start with the data generating process:\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{y})^2] = \\mathbb{E}[(f(x) + \\epsilon - \\hat{y})^2].\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(f(x) - \\hat{y} + \\epsilon)^2] \n",
    "= \\mathbb{E}[(f(x) - \\hat{y})^2] + 2\\mathbb{E}[(f(x) - \\hat{y})\\epsilon] + \\mathbb{E}[\\epsilon^2].\n",
    "$$\n",
    "\n",
    "Since $\\epsilon$ is independent with zero mean, the cross-term vanishes:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{y})^2] = \\mathbb{E}[(f(x) - \\hat{y})^2] + \\sigma^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Now decompose the first term by adding and subtracting $\\mathbb{E}[\\hat{y}]$ ((Hastie, Tibshirani, and Friedman, 2009, p.223)):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(f(x) - \\hat{y})^2] \n",
    "= \\mathbb{E}\\big[(f(x) - \\mathbb{E}[\\hat{y}] + \\mathbb{E}[\\hat{y}] - \\hat{y})^2\\big].\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}\\big[(f(x) - \\mathbb{E}[\\hat{y}])^2\\big] \n",
    "+ \\mathbb{E}\\big[(\\hat{y} - \\mathbb{E}[\\hat{y}])^2\\big] \n",
    "+ 2\\mathbb{E}\\big[(f(x) - \\mathbb{E}[\\hat{y}])(\\mathbb{E}[\\hat{y}] - \\hat{y})\\big].\n",
    "$$\n",
    "\n",
    "The cross-term vanishes because $(f(x) - \\mathbb{E}[\\hat{y}])$ is constant w.r.t. the randomness in $\\hat{y}$. So:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(f(x) - \\hat{y})^2] \n",
    "= (f(x) - \\mathbb{E}[\\hat{y}])^2 + \\operatorname{Var}(\\hat{y}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Thus the full decomposition is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{y})^2] \n",
    "= (f(x) - \\mathbb{E}[\\hat{y}])^2 + \\operatorname{Var}(\\hat{y}) + \\sigma^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Identify terms:\n",
    "\n",
    "- Bias: $\\operatorname{Bias}[\\hat{y}]^2 = (f(x) - \\mathbb{E}[\\hat{y}])^2$  \n",
    "- Variance: $\\operatorname{Var}[\\hat{y}] = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[\\hat{y}])^2]$  \n",
    "- Nondeterministic noise: $\\sigma^2$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{y})^2] \n",
    "= \\operatorname{Bias}[\\hat{y}]^2 + \\operatorname{Var}[\\hat{y}] + \\sigma^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bfe19",
   "metadata": {},
   "source": [
    "**b)** Explain what the terms mean and discuss their interpretations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a7966",
   "metadata": {},
   "source": [
    "The decomposition above splits predictive error into three conceptual parts. \n",
    "- The irreducibile error comes from the fact of sampling, no matter how we well we estimate $f(x)$ noise is assumed and this variance cannot be reduce unless $\\epsilon$ is assumed $\\sim \\sigma^2 = 0$\n",
    "- Bias can be understood as systematic error, or the squared distance between the prediction expectation and the \"true function\" (underlying data-generating mechanism). Importantly, if the model class (e.g linear model) cannot represent the true function (e.g nonlinear function), bias is high. The more flexible the model is, the more it can reduce bias. \n",
    "\n",
    "Forexample, for basis expansions (polynomials, splines), the model bias can be made arbitrarily small by letting the number of basis functions $p$ grow, since such bases are dense in the space of continuous functions (Hastie et al., 2009, 233).\n",
    "\n",
    "- Variance: sensitivity of the estimator to sampling fluctuations. It measures how much $\\hat{y}$ (determined by the estimator and noise) varies around its mean $\\mathbb{E}[\\hat{y}]$ when the training data changes. High variance means the model overfits to noise in the training set.\n",
    "\n",
    "Hence, in most cases, the total mean squared error is the main metric for model assessment. This is an overarching metric combining the tradeoffs central for model assessment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
