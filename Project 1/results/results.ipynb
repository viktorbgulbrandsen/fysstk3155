{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fca86d2",
   "metadata": {},
   "source": [
    "### Part a : Ordinary Least Square (OLS) for the Runge function\n",
    "\n",
    "*Write your own code* (using for example the  pseudoinverse function **pinv** from  **Numpy** ) and perform a standard **ordinary least square regression**\n",
    "analysis using polynomials in $x$ up to  order $15$ or higher. Explore the dependence on the number of data points and the polynomial degree.\n",
    "\n",
    "Evaluate the mean Squared error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4b509",
   "metadata": {},
   "source": [
    "---\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1521cec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 5:\n",
      "MSE: 0.447822\n",
      "R²: -0.056579\n",
      "Parameters: [ 0.14532937 -0.32074898 -0.35453549]...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from data_processing import RungeData, create_polynomial_features\n",
    "from regression_models import OLSRegression\n",
    "from evaluation_metrics import mse, r2_score\n",
    "\n",
    "runge_data = RungeData(n_points=100)\n",
    "runge_data.add_noise()\n",
    "runge_data.scale()\n",
    "\n",
    "X_train, X_test, y_train, y_test = runge_data.split()\n",
    "\n",
    "degree = 5\n",
    "X_train_poly = create_polynomial_features(X_train, degree)\n",
    "X_test_poly = create_polynomial_features(X_test, degree)\n",
    "\n",
    "model = OLSRegression()\n",
    "model.fit(X_train_poly, y_train, method='pinv')\n",
    "\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "print(f\"Degree {degree}:\")\n",
    "print(f\"MSE: {mse(y_test, y_pred):.6f}\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.6f}\")\n",
    "print(f\"Parameters: {model.theta[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714561ae",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Todo:\n",
    "\n",
    "Appropriate explaination on OLS scaling invariance from\n",
    "https://github.com/viktorbgulbrandsen/fysstk3155/blob/main/uke38/exercises.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fo8i2alaf5",
   "metadata": {},
   "source": [
    "### Part b : Ridge regression for the Runge function\n",
    "\n",
    "Ridge regression with different regularization methods for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93zeqa7mt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Results:\n",
      "Lambda\t\tMSE\t\tR²\n",
      "-----------------------------------\n",
      "0.001\t\t0.447816\t-0.056565\n",
      "0.100\t\t0.447356\t-0.055479\n",
      "1.000\t\t0.447420\t-0.055629\n",
      "10.000\t\t0.450419\t-0.062707\n",
      "100.000\t\t0.438751\t-0.035176\n",
      "\n",
      "Compare with OLS:\n",
      "OLS\t\t0.447822\t-0.056579\n",
      "\n",
      "Parameter magnitude comparison:\n",
      "OLS max |θ|: 0.7787\n",
      "Ridge (λ=0.1) max |θ|: 0.7294\n",
      "Ridge (λ=10.0) max |θ|: 0.1463\n"
     ]
    }
   ],
   "source": [
    "from data_processing import create_polynomial_features\n",
    "from regression_models import RidgeRegression\n",
    "import numpy as np\n",
    "\n",
    "ridge_data = RungeData(n_points=100)\n",
    "ridge_data.add_noise()\n",
    "ridge_data.scale()\n",
    "\n",
    "X_train, X_test, y_train, y_test = ridge_data.split()\n",
    "\n",
    "degree = 5\n",
    "X_train_poly = create_polynomial_features(X_train, degree)\n",
    "X_test_poly = create_polynomial_features(X_test, degree)\n",
    "\n",
    "lambdas = [0.001, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"Ridge Regression Results:\")\n",
    "print(\"Lambda\\t\\tMSE\\t\\tR²\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for lmbda in lambdas:\n",
    "    ridge_model = RidgeRegression()\n",
    "    ridge_model.fit(X_train_poly, y_train, lmbda=lmbda)\n",
    "    \n",
    "    y_pred = ridge_model.predict(X_test_poly)\n",
    "    \n",
    "    mse_val = mse(y_test, y_pred)\n",
    "    r2_val = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{lmbda:.3f}\\t\\t{mse_val:.6f}\\t{r2_val:.6f}\")\n",
    "\n",
    "print(\"\\nCompare with OLS:\")\n",
    "ols_model = OLSRegression()\n",
    "ols_model.fit(X_train_poly, y_train, method='pinv')\n",
    "y_pred_ols = ols_model.predict(X_test_poly)\n",
    "\n",
    "print(f\"OLS\\t\\t{mse(y_test, y_pred_ols):.6f}\\t{r2_score(y_test, y_pred_ols):.6f}\")\n",
    "\n",
    "ridge_small = RidgeRegression()\n",
    "ridge_small.fit(X_train_poly, y_train, lmbda=0.1)\n",
    "\n",
    "ridge_large = RidgeRegression()\n",
    "ridge_large.fit(X_train_poly, y_train, lmbda=10.0)\n",
    "\n",
    "print(f\"\\nParameter magnitude comparison:\")\n",
    "print(f\"OLS max |θ|: {np.max(np.abs(ols_model.theta)):.4f}\")\n",
    "print(f\"Ridge (λ=0.1) max |θ|: {np.max(np.abs(ridge_small.theta)):.4f}\")\n",
    "print(f\"Ridge (λ=10.0) max |θ|: {np.max(np.abs(ridge_large.theta)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "u9npat18ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge method comparison (λ=1.0):\n",
      "Method\t\tMSE\t\tR²\n",
      "-----------------------------------\n",
      "analytical\t0.447420\t-0.055629\n",
      "cholesky\t0.447420\t-0.055629\n",
      "\n",
      "Note: Both methods should give identical results\n",
      "Cholesky method using np.linalg.solve() is more numerically stable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRidge method comparison (λ=1.0):\")\n",
    "print(\"Method\\t\\tMSE\\t\\tR²\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "methods = ['analytical', 'cholesky']\n",
    "\n",
    "for method in methods:\n",
    "    ridge_method = RidgeRegression()\n",
    "    ridge_method.fit(X_train_poly, y_train, lmbda=1.0, method=method)\n",
    "    \n",
    "    y_pred_method = ridge_method.predict(X_test_poly)\n",
    "    \n",
    "    mse_val = mse(y_test, y_pred_method)\n",
    "    r2_val = r2_score(y_test, y_pred_method)\n",
    "    \n",
    "    print(f\"{method}\\t{mse_val:.6f}\\t{r2_val:.6f}\")\n",
    "    \n",
    "print(\"\\nNote: Both methods should give identical results\")\n",
    "print(\"Cholesky method using np.linalg.solve() is more numerically stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ispkkum0npl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO Regression Results:\n",
      "Lambda\t\tMSE\t\tR2\t\tNon-zero params\n",
      "--------------------------------------------------\n",
      "0.001\t\t0.454026\t-0.071216\t6\n",
      "0.010\t\t0.459765\t-0.084757\t6\n",
      "0.100\t\t0.438133\t-0.033719\t6\n",
      "1.000\t\t0.436964\t-0.030961\t6\n",
      "10.000\t\t0.468867\t-0.106232\t6\n",
      "\n",
      "Parameter sparsity comparison (λ=0.1):\n",
      "LASSO max |θ|: 0.0198\n",
      "LASSO non-zero params: 6\n",
      "Ridge max |θ|: 0.7294\n",
      "Ridge non-zero params: 6\n",
      "\n",
      "Note: LASSO sets many parameters to exactly zero (feature selection)\n",
      "Ridge shrinks parameters but keeps them non-zero\n"
     ]
    }
   ],
   "source": [
    "from data_processing import create_polynomial_features\n",
    "from regression_models import LassoRegression, RidgeRegression\n",
    "from gradient_descent import GradientDescent\n",
    "import numpy as np\n",
    "\n",
    "lasso_data = RungeData(n_points=100)\n",
    "lasso_data.add_noise()\n",
    "lasso_data.scale()\n",
    "\n",
    "X_train, X_test, y_train, y_test = lasso_data.split()\n",
    "\n",
    "degree = 5\n",
    "X_train_poly = create_polynomial_features(X_train, degree)\n",
    "X_test_poly = create_polynomial_features(X_test, degree)\n",
    "\n",
    "lambdas = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "print(\"LASSO Regression Results:\")\n",
    "print(\"Lambda\\t\\tMSE\\t\\tR2\\t\\tNon-zero params\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lmbda in lambdas:\n",
    "    lasso = LassoRegression(lmbda=lmbda)\n",
    "    optimizer = GradientDescent()\n",
    "    \n",
    "    optimizer.optimize(lasso, X_train_poly, y_train, learning_rate=0.01, max_iter=1000)\n",
    "    \n",
    "    y_pred = lasso.predict(X_test_poly)\n",
    "    \n",
    "    mse_val = mse(y_test, y_pred)\n",
    "    r2_val = r2_score(y_test, y_pred)\n",
    "    non_zero_params = np.sum(np.abs(lasso.theta) > 1e-4)\n",
    "    \n",
    "    print(f\"{lmbda:.3f}\\t\\t{mse_val:.6f}\\t{r2_val:.6f}\\t{non_zero_params}\")\n",
    "\n",
    "print(f\"\\nParameter sparsity comparison (λ=0.1):\")\n",
    "\n",
    "lasso_sparse = LassoRegression(lmbda=0.1)\n",
    "optimizer.optimize(lasso_sparse, X_train_poly, y_train)\n",
    "\n",
    "ridge_compare = RidgeRegression()\n",
    "ridge_compare.fit(X_train_poly, y_train, lmbda=0.1)\n",
    "\n",
    "print(f\"LASSO max |θ|: {np.max(np.abs(lasso_sparse.theta)):.4f}\")\n",
    "print(f\"LASSO non-zero params: {np.sum(np.abs(lasso_sparse.theta) > 1e-4)}\")\n",
    "print(f\"Ridge max |θ|: {np.max(np.abs(ridge_compare.theta)):.4f}\") \n",
    "print(f\"Ridge non-zero params: {np.sum(np.abs(ridge_compare.theta) > 1e-4)}\")\n",
    "\n",
    "print(f\"\\nNote: LASSO sets many parameters to exactly zero (feature selection)\")\n",
    "print(f\"Ridge shrinks parameters but keeps them non-zero\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
